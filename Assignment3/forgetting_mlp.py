# -*- coding: utf-8 -*-
"""depth_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wLiLzQ7U2XYVX7W5ajOAm9PpLuWdrnx0
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
from torch.utils.data import DataLoader, TensorDataset


num_tasks = 10
batch_size = 32
learning_rate = 0.001
input_size = 784
neurons = 256
output = 10


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

mnist_train = datasets.MNIST(root='./data', train=True, download=True,
                             transform=transforms.ToTensor())
mnist_test = datasets.MNIST(root='./data', train=False, download=True,
                            transform=transforms.ToTensor())

X_train = mnist_train.data.view(-1, 784).float() / 255.0
y_train = mnist_train.targets
X_test = mnist_test.data.view(-1, 784).float() / 255.0
y_test = mnist_test.targets


permutations = [torch.tensor(np.random.permutation(input_size)) for i in range(num_tasks)]


def get_task_dataset(X, y, permutation):
    X_perm = X[:, permutation]          # reorder columns in one shot
    return TensorDataset(X_perm, y)

task_datasets = []
for curr_perm in permutations:
    train_dataset = get_task_dataset(X_train, y_train, curr_perm)
    test_dataset  = get_task_dataset(X_test,  y_test,  curr_perm)
    task_datasets.append((train_dataset, test_dataset))


class MLP(nn.Module):
    def __init__(self, input_size, neurons, output):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, neurons),
            nn.ReLU(),
            nn.Linear(neurons, output),
            nn.LogSoftmax(dim=1)
        )
    def forward(self, x):
        return self.net(x)

def train_task(model, train_loader, optimizer, criterion, epochs):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"   Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}")

def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return correct / total


model = MLP(input_size, neurons, output).to(device)
criterion = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)



task_id=0
for train_dataset, test_dataset in task_datasets:
    print(f"\n=== Training on Task {task_id+1} ===")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    if task_id==0:
        train_task(model, train_loader, optimizer, criterion, epochs=50)
    else:
        train_task(model, train_loader, optimizer, criterion, epochs=20)


    for i in range(task_id + 1):
        test_loader = DataLoader(task_datasets[i][1], batch_size=batch_size, shuffle=False)
        acc = evaluate(model, test_loader)
        print(f"Accuracy on Task {i+1}: {acc*100:.2f}%")
    task_id+=1

"""depth_2"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
from torch.utils.data import DataLoader, TensorDataset


num_tasks = 10
batch_size = 32
learning_rate = 0.001
input_size = 784
neurons = 256
output = 10


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

mnist_train = datasets.MNIST(root='./data', train=True, download=True,
                             transform=transforms.ToTensor())
mnist_test = datasets.MNIST(root='./data', train=False, download=True,
                            transform=transforms.ToTensor())

X_train = mnist_train.data.view(-1, 784).float() / 255.0
y_train = mnist_train.targets
X_test = mnist_test.data.view(-1, 784).float() / 255.0
y_test = mnist_test.targets


permutations = [torch.tensor(np.random.permutation(input_size)) for i in range(num_tasks)]


def get_task_dataset(X, y, permutation):
    X_perm = X[:, permutation]          # reorder columns in one shot
    return TensorDataset(X_perm, y)

task_datasets = []
for curr_perm in permutations:
    train_dataset = get_task_dataset(X_train, y_train, curr_perm)
    test_dataset  = get_task_dataset(X_test,  y_test,  curr_perm)
    task_datasets.append((train_dataset, test_dataset))


class MLP(nn.Module):
    def __init__(self, input_size, neurons, output):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, neurons),
            nn.ReLU(),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Linear(neurons, output),
            nn.LogSoftmax(dim=1)
        )
    def forward(self, x):
        return self.net(x)

def train_task(model, train_loader, optimizer, criterion, epochs):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"   Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}")

def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return correct / total


model = MLP(input_size, neurons, output).to(device)
criterion = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate)




task_id=0
for train_dataset, test_dataset in task_datasets:
    print(f"\n=== Training on Task {task_id+1} ===")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    if task_id==0:
        train_task(model, train_loader, optimizer, criterion, epochs=50)
    else:
        train_task(model, train_loader, optimizer, criterion, epochs=20)


    for i in range(task_id + 1):
        test_loader = DataLoader(task_datasets[i][1], batch_size=batch_size, shuffle=False)
        acc = evaluate(model, test_loader)
        print(f"Accuracy on Task {i+1}: {acc*100:.2f}%")
    task_id+=1

"""depth 3 SGD"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

num_tasks = 10
batch_size = 32
learning_rate = 0.001
input_size = 784
neurons = 256
output = 10

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

mnist_train = datasets.MNIST(root='./data', train=True, download=True,
                             transform=transforms.ToTensor())
mnist_test = datasets.MNIST(root='./data', train=False, download=True,
                            transform=transforms.ToTensor())

X_train = mnist_train.data.view(-1, 784).float() / 255.0
y_train = mnist_train.targets
X_test = mnist_test.data.view(-1, 784).float() / 255.0
y_test = mnist_test.targets

permutations = [torch.tensor(np.random.permutation(input_size)) for i in range(num_tasks)]

def get_task_dataset(X, y, permutation):
    X_perm = X[:, permutation]          # reorder columns in one shot
    return TensorDataset(X_perm, y)

task_datasets = []
for curr_perm in permutations:
    train_dataset = get_task_dataset(X_train, y_train, curr_perm)
    test_dataset  = get_task_dataset(X_test,  y_test,  curr_perm)
    task_datasets.append((train_dataset, test_dataset))
class MLP(nn.Module):
    def __init__(self, input_size, neurons, output):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, neurons),
            nn.ReLU(),
            nn.Dropout(p=0.5),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Dropout(p=0.5),
            nn.Linear(neurons, output),
            nn.LogSoftmax(dim=1)
        )
    def forward(self, x):
        return self.net(x)


def train_task(model, train_loader, test_loader, optimizer, criterion, epochs, task_id, val_history):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # evaluate at the end of each epoch
        acc = evaluate(model, test_loader)
        val_history[task_id].append(acc * 100)

        print(f"   Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Val Acc: {acc*100:.2f}%")

def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return correct / total

model = MLP(input_size, neurons, output).to(device)
criterion = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# dictionary to store validation accuracies per epoch per task
val_history = {i: [] for i in range(num_tasks)}

task_id=0
for train_dataset, test_dataset in task_datasets:
    print(f"\n=== Training on Task {task_id+1} ===")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    if task_id==0:
        train_task(model, train_loader, test_loader, optimizer, criterion, epochs=50, task_id=task_id, val_history=val_history)
    else:
        train_task(model, train_loader, test_loader, optimizer, criterion, epochs=20, task_id=task_id, val_history=val_history)

    for i in range(task_id + 1):
        test_loader = DataLoader(task_datasets[i][1], batch_size=batch_size, shuffle=False)
        acc = evaluate(model, test_loader)
        print(f"Accuracy on Task {i+1}: {acc*100:.2f}%")
    task_id+=1


plt.figure(figsize=(10,6))
for task_id, acc_list in val_history.items():
    if len(acc_list) > 0:   # plot only trained tasks
        plt.plot(range(1, len(acc_list)+1), acc_list, label=f"Task {task_id+1}")

plt.xlabel("Epochs")
plt.ylabel("Validation Accuracy (%)")
plt.title("Validation Accuracy per Task")
plt.legend()
plt.grid(True)
plt.show()

"""depth 3 dropout <=0.5"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import torch.nn.functional as F

num_tasks = 10
batch_size = 32
learning_rate = 0.001
input_size = 784
neurons = 256
output = 10

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

mnist_train = datasets.MNIST(root='./data', train=True, download=True,
                             transform=transforms.ToTensor())
mnist_test = datasets.MNIST(root='./data', train=False, download=True,
                            transform=transforms.ToTensor())

X_train = mnist_train.data.view(-1, 784).float() / 255.0
y_train = mnist_train.targets
X_test = mnist_test.data.view(-1, 784).float() / 255.0
y_test = mnist_test.targets

permutations = [torch.tensor(np.random.permutation(input_size)) for i in range(num_tasks)]

def get_task_dataset(X, y, permutation):
    X_perm = X[:, permutation]          # reorder columns in one shot
    return TensorDataset(X_perm, y)

task_datasets = []
for curr_perm in permutations:
    train_dataset = get_task_dataset(X_train, y_train, curr_perm)
    test_dataset  = get_task_dataset(X_test,  y_test,  curr_perm)
    task_datasets.append((train_dataset, test_dataset))

# -------- MLP without LogSoftmax --------
class MLP(nn.Module):
    def __init__(self, input_size, neurons, output):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, neurons),
            nn.ReLU(),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Linear(neurons, output)
        )
    def forward(self, x):
        return self.net(x)

# -------- Training with MSELoss --------
def train_task(model, train_loader, test_loader, optimizer, criterion, epochs, task_id, acc_matrix):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()

            outputs = model(batch_x)              # logits
            probs = F.softmax(outputs, dim=1)     # convert to probabilities

            # one-hot encode labels
            one_hot = F.one_hot(batch_y, num_classes=output).float()

            # compute MSE loss
            loss = criterion(probs, one_hot)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        acc = evaluate(model, test_loader)
        acc_matrix[task_id].append(acc * 100)
        print(f"   Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Val Acc: {acc*100:.2f}%")

def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            probs = F.softmax(outputs, dim=1)
            _, predicted = torch.max(probs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return correct / total

model = MLP(input_size, neurons, output).to(device)
criterion = nn.MSELoss()   # <-- L2 loss
optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)

# dictionary to store accuracies across tasks
acc_matrix = {i: [] for i in range(num_tasks)}

task_id = 0
for train_dataset, test_dataset in task_datasets:
    print(f"\n=== Training on Task {task_id+1} ===")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    if task_id == 0:
        train_task(model, train_loader, test_loader, optimizer, criterion, epochs=50, task_id=task_id, acc_matrix=acc_matrix)
    else:
        train_task(model, train_loader, test_loader, optimizer, criterion, epochs=20, task_id=task_id, acc_matrix=acc_matrix)

    for i in range(task_id + 1):
        test_loader = DataLoader(task_datasets[i][1], batch_size=batch_size, shuffle=False)
        acc = evaluate(model, test_loader)
        acc_matrix[i].append(acc * 100)
        print(f"Accuracy on Task {i+1}: {acc*100:.2f}%")
    task_id += 1

# ---- plotting ----
plt.figure(figsize=(10,6))
for i, acc_list in acc_matrix.items():
    if len(acc_list) > 0:
        plt.plot(range(i+1, num_tasks+1), acc_list, marker='o', label=f"Task {i+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curve Across Tasks (MSE / L2 Loss)")
plt.legend()
plt.grid(True)
plt.show()

"""L2"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import random

# -------------------------
# Settings
# -------------------------
num_tasks = 10
batch_size = 32
learning_rate = 0.001
input_size = 784
neurons = 256
output = 10

# ðŸ”¹ Unique seed value for reproducibility
seed_value = 42   # change this to your student ID or any unique number
torch.manual_seed(seed_value)
np.random.seed(seed_value)
random.seed(seed_value)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed_value)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# -------------------------
# Data
# -------------------------
mnist_train = datasets.MNIST(root='./data', train=True, download=True,
                             transform=transforms.ToTensor())
mnist_test = datasets.MNIST(root='./data', train=False, download=True,
                            transform=transforms.ToTensor())

X_train = mnist_train.data.view(-1, 784).float() / 255.0
y_train = mnist_train.targets
X_test = mnist_test.data.view(-1, 784).float() / 255.0
y_test = mnist_test.targets

permutations = [torch.tensor(np.random.permutation(input_size)) for i in range(num_tasks)]

def get_task_dataset(X, y, permutation):
    X_perm = X[:, permutation]          # reorder columns in one shot
    return TensorDataset(X_perm, y)

task_datasets = []
for curr_perm in permutations:
    train_dataset = get_task_dataset(X_train, y_train, curr_perm)
    test_dataset  = get_task_dataset(X_test,  y_test,  curr_perm)
    task_datasets.append((train_dataset, test_dataset))

# -------------------------
# Model
# -------------------------
class MLP(nn.Module):
    def __init__(self, input_size, neurons, output):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, neurons),
            nn.ReLU(),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Linear(neurons, output),
            nn.LogSoftmax(dim=1)   # ðŸ”¹ needed for NLLLoss
        )
    def forward(self, x):
        return self.net(x)

# -------------------------
# Training / Evaluation
# -------------------------
def train_task(model, train_loader, test_loader, optimizer, criterion, epochs, task_id, val_history):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)

            optimizer.zero_grad()
            outputs = model(batch_x)

            loss = criterion(outputs, batch_y)  # ðŸ”¹ NLLLoss expects class indices
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # validation accuracy
        acc = evaluate(model, test_loader)
        val_history[task_id].append(acc * 100)
        print(f"   Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Val Acc: {acc*100:.2f}%")

def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return correct / total

# -------------------------
# Setup Model + NLL Loss
# -------------------------
model = MLP(input_size, neurons, output).to(device)
criterion = nn.NLLLoss()   # ðŸ”¹ back to NLLLoss
optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)

# -------------------------
# Store validation accuracies per epoch
# -------------------------
val_history = {i: [] for i in range(num_tasks)}   # initialize once
acc_matrix = {i: [] for i in range(num_tasks)}    # for forgetting curve

task_id = 0
for train_dataset, test_dataset in task_datasets:
    print(f"\n=== Training on Task {task_id+1} ===")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    if task_id == 0:
        train_task(model, train_loader, DataLoader(test_dataset, batch_size=batch_size),
                   optimizer, criterion, epochs=50, task_id=task_id, val_history=val_history)
    else:
        train_task(model, train_loader, DataLoader(test_dataset, batch_size=batch_size),
                   optimizer, criterion, epochs=20, task_id=task_id, val_history=val_history)

    # Evaluate on all tasks up to now
    for i in range(task_id + 1):
        test_loader = DataLoader(task_datasets[i][1], batch_size=batch_size, shuffle=False)
        acc = evaluate(model, test_loader)
        acc_matrix[i].append(acc * 100)
        print(f"Accuracy on Task {i+1}: {acc*100:.2f}%")

    task_id += 1

# -------------------------
# Plot Forgetting Curve
# -------------------------
plt.figure(figsize=(10,6))
for i, acc_list in acc_matrix.items():
    if len(acc_list) > 0:
        plt.plot(range(i+1, num_tasks+1), acc_list, marker='o', label=f"Task {i+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curve Across Tasks (NLL Loss)")
plt.legend()
plt.grid(True)
plt.show()

"""Seed"""

import matplotlib.pyplot as plt
import numpy as np

# ---- Hardcoded accuracy matrix from your output ----
acc_matrix = np.array([
    [98.15,   0,     0,     0,     0,     0,     0,     0,     0,     0],  # after Task 1
    [88.61, 97.57,   0,     0,     0,     0,     0,     0,     0,     0],  # after Task 2
    [55.12, 85.60, 97.59,   0,     0,     0,     0,     0,     0,     0],  # after Task 3
    [31.16, 61.89, 82.99, 97.13,   0,     0,     0,     0,     0,     0],  # after Task 4
    [31.92, 50.77, 72.46, 78.90, 97.65,   0,     0,     0,     0,     0],  # after Task 5
    [19.03, 33.46, 56.73, 67.15, 84.60, 97.28,   0,     0,     0,     0],  # after Task 6
    [18.22, 26.15, 42.73, 45.45, 61.74, 82.85, 97.06,   0,     0,     0],  # after Task 7
    [19.99, 22.93, 27.05, 33.78, 49.50, 57.64, 82.46, 96.87,   0,     0],  # after Task 8
    [15.33, 17.55, 20.84, 30.80, 34.19, 46.84, 65.38, 80.34, 96.88,   0],  # after Task 9
    [20.22, 16.61, 19.62, 26.13, 27.64, 36.32, 50.28, 62.17, 85.69, 96.83] # after Task 10
])



T = acc_matrix.shape[0]

# ---- Metrics ----
ACC = np.mean(acc_matrix[-1])

# Backward Transfer (BWT)
BWT = np.mean([acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)])

# Task-wise BWT (TBWT) - per task forgetting
TBWT = [acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)]

# Cumulative BWT (CBWT) - average of TBWT
CBWT = np.mean(TBWT)

# ---- Print Results ----
print(f"ACC (Final Average Accuracy): {ACC:.2f}%")
print(f"BWT (Backward Transfer): {BWT:.2f}%")
print(f"TBWT (Task-wise BWT): {TBWT}")
print(f"CBWT (Cumulative BWT): {CBWT:.2f}%")

# ---- Plot Forgetting Curves ----
plt.figure(figsize=(12,7))
for task in range(T):
    valid_accs = acc_matrix[task:, task]
    plt.plot(range(task+1, T+1), valid_accs, marker='o', label=f"Task {task+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curves for Each Task")
plt.legend()
plt.grid(True)
plt.show()

"""Depth"""

import matplotlib.pyplot as plt
import numpy as np

# ---- Hardcoded accuracy matrix from your output ----
acc_matrix = np.array([
    [94.06,   0,     0,     0,     0,     0,     0,     0,     0,     0],   # after Task 1
    [91.35, 93.62,   0,     0,     0,     0,     0,     0,     0,     0],   # after Task 2
    [90.61, 89.83, 94.37,   0,     0,     0,     0,     0,     0,     0],   # after Task 3
    [88.88, 87.39, 90.07, 94.89,   0,     0,     0,     0,     0,     0],   # after Task 4
    [84.80, 84.01, 86.15, 93.20, 95.01,   0,     0,     0,     0,     0],   # after Task 5
    [80.73, 73.65, 76.05, 86.47, 93.30, 95.19,   0,     0,     0,     0],   # after Task 6
    [77.54, 70.82, 75.50, 82.50, 87.05, 92.22, 95.32,   0,     0,     0],   # after Task 7
    [72.91, 65.20, 69.65, 74.04, 79.50, 86.88, 93.88, 95.55,   0,     0],   # after Task 8
    [70.47, 69.21, 58.21, 69.36, 69.91, 82.14, 84.93, 93.49, 95.76,   0],   # after Task 9
    [60.47, 69.79, 46.65, 55.95, 60.95, 73.65, 85.61, 91.31, 90.48, 95.70]  # after Task 10
])



T = acc_matrix.shape[0]

# ---- Metrics ----
ACC = np.mean(acc_matrix[-1])

# Backward Transfer (BWT)
BWT = np.mean([acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)])

# Task-wise BWT (TBWT) - per task forgetting
TBWT = [acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)]

# Cumulative BWT (CBWT) - average of TBWT
CBWT = np.mean(TBWT)

# ---- Print Results ----
print(f"ACC (Final Average Accuracy): {ACC:.2f}%")
print(f"BWT (Backward Transfer): {BWT:.2f}%")
print(f"TBWT (Task-wise BWT): {TBWT}")
print(f"CBWT (Cumulative BWT): {CBWT:.2f}%")

# ---- Plot Forgetting Curves ----
plt.figure(figsize=(12,7))
for task in range(T):
    valid_accs = acc_matrix[task:, task]
    plt.plot(range(task+1, T+1), valid_accs, marker='o', label=f"Task {task+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curves for Each Task")
plt.legend()
plt.grid(True)
plt.show()

"""SGD"""

import matplotlib.pyplot as plt
import numpy as np

acc_matrix = np.array([
    [98.09,   0,     0,     0,     0,     0,     0,     0,     0,     0],   # after Task 1
    [87.20, 97.62,   0,     0,     0,     0,     0,     0,     0,     0],   # after Task 2
    [59.83, 79.36, 97.50,   0,     0,     0,     0,     0,     0,     0],   # after Task 3
    [42.33, 50.22, 76.70, 97.24,   0,     0,     0,     0,     0,     0],   # after Task 4
    [30.22, 33.46, 58.61, 75.99, 97.19,   0,     0,     0,     0,     0],   # after Task 5
    [30.83, 23.24, 40.53, 40.18, 77.73, 96.65,   0,     0,     0,     0],   # after Task 6
    [28.18, 16.53, 20.62, 20.35, 51.00, 73.02, 96.75,   0,     0,     0],   # after Task 7
    [20.35, 12.43, 21.09, 12.39, 31.68, 43.19, 66.01, 97.12,   0,     0],   # after Task 8
    [19.02, 11.79, 12.64, 11.70, 20.97, 34.15, 39.78, 74.60, 96.87,   0],   # after Task 9
    [12.45, 11.55, 15.97, 13.37, 15.06, 24.84, 27.42, 37.35, 65.65, 97.04]  # after Task 10
])



T = acc_matrix.shape[0]

# ---- Metrics ----
ACC = np.mean(acc_matrix[-1])

# Backward Transfer (BWT)
BWT = np.mean([acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)])

# Task-wise BWT (TBWT) - per task forgetting
TBWT = [acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)]

# Cumulative BWT (CBWT) - average of TBWT
CBWT = np.mean(TBWT)

# ---- Print Results ----
print(f"ACC (Final Average Accuracy): {ACC:.2f}%")
print(f"BWT (Backward Transfer): {BWT:.2f}%")
print(f"TBWT (Task-wise BWT): {TBWT}")
print(f"CBWT (Cumulative BWT): {CBWT:.2f}%")

# ---- Plot Forgetting Curves ----
plt.figure(figsize=(12,7))
for task in range(T):
    valid_accs = acc_matrix[task:, task]
    plt.plot(range(task+1, T+1), valid_accs, marker='o', label=f"Task {task+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curves for Each Task")
plt.legend()
plt.grid(True)
plt.show()

"""Dropout"""

import matplotlib.pyplot as plt
import numpy as np



acc_matrix = np.array([
    [98.08,   0,     0,     0,     0,     0,     0,     0,     0,     0],   # after Task 1
    [89.91, 97.36,   0,     0,     0,     0,     0,     0,     0,     0],   # after Task 2
    [68.50, 83.43, 97.72,   0,     0,     0,     0,     0,     0,     0],   # after Task 3
    [56.58, 57.23, 81.99, 97.68,   0,     0,     0,     0,     0,     0],   # after Task 4
    [31.19, 29.98, 54.87, 82.41, 97.55,   0,     0,     0,     0,     0],   # after Task 5
    [25.37, 26.03, 41.71, 66.83, 88.95, 97.18,   0,     0,     0,     0],   # after Task 6
    [23.77, 25.24, 37.53, 41.43, 64.60, 82.75, 97.33,   0,     0,     0],   # after Task 7
    [27.30, 15.37, 27.51, 37.02, 46.62, 61.31, 83.54, 97.18,   0,     0],   # after Task 8
    [13.12, 10.76, 22.50, 22.48, 29.56, 45.18, 59.43, 87.22, 97.06,   0],   # after Task 9
    [14.10,  9.12, 17.90, 21.93, 26.61, 31.12, 38.30, 54.47, 85.65, 96.43]  # after Task 10
])



T = acc_matrix.shape[0]

# ---- Metrics ----
ACC = np.mean(acc_matrix[-1])

# Backward Transfer (BWT)
BWT = np.mean([acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)])

# Task-wise BWT (TBWT) - per task forgetting
TBWT = [acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)]

# Cumulative BWT (CBWT) - average of TBWT
CBWT = np.mean(TBWT)

# ---- Print Results ----
print(f"ACC (Final Average Accuracy): {ACC:.2f}%")
print(f"BWT (Backward Transfer): {BWT:.2f}%")
print(f"TBWT (Task-wise BWT): {TBWT}")
print(f"CBWT (Cumulative BWT): {CBWT:.2f}%")

# ---- Plot Forgetting Curves ----
plt.figure(figsize=(12,7))
for task in range(T):
    valid_accs = acc_matrix[task:, task]
    plt.plot(range(task+1, T+1), valid_accs, marker='o', label=f"Task {task+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curves for Each Task")
plt.legend()
plt.grid(True)
plt.show()

"""L2"""

import matplotlib.pyplot as plt
import numpy as np



acc_matrix = np.array([
    [97.94,   0,     0,     0,     0,     0,     0,     0,     0,     0],   # after Task 1
    [74.39, 97.73,   0,     0,     0,     0,     0,     0,     0,     0],   # after Task 2
    [55.02, 72.67, 97.79,   0,     0,     0,     0,     0,     0,     0],   # after Task 3
    [36.00, 45.31, 66.13, 97.33,   0,     0,     0,     0,     0,     0],   # after Task 4
    [22.30, 34.27, 49.84, 85.95, 97.43,   0,     0,     0,     0,     0],   # after Task 5
    [10.08, 24.99, 36.98, 59.19, 73.87, 97.27,   0,     0,     0,     0],   # after Task 6
    [20.43, 21.82, 22.09, 35.88, 40.66, 67.82, 97.51,   0,     0,     0],   # after Task 7
    [16.66, 17.61, 21.09, 25.13, 34.89, 47.66, 85.86, 97.01,   0,     0],   # after Task 8
    [18.80, 15.66, 19.30, 29.32, 33.62, 44.31, 55.40, 73.62, 97.32,   0],   # after Task 9
    [16.26, 15.04, 17.31, 22.93, 21.95, 32.06, 38.73, 44.45, 73.07, 97.12]  # after Task 10
])



T = acc_matrix.shape[0]

# ---- Metrics ----
ACC = np.mean(acc_matrix[-1])

# Backward Transfer (BWT)
BWT = np.mean([acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)])

# Task-wise BWT (TBWT) - per task forgetting
TBWT = [acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)]

# Cumulative BWT (CBWT) - average of TBWT
CBWT = np.mean(TBWT)

# ---- Print Results ----
print(f"ACC (Final Average Accuracy): {ACC:.2f}%")
print(f"BWT (Backward Transfer): {BWT:.2f}%")
print(f"TBWT (Task-wise BWT): {TBWT}")
print(f"CBWT (Cumulative BWT): {CBWT:.2f}%")

# ---- Plot Forgetting Curves ----
plt.figure(figsize=(12,7))
for task in range(T):
    valid_accs = acc_matrix[task:, task]
    plt.plot(range(task+1, T+1), valid_accs, marker='o', label=f"Task {task+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curves for Each Task")
plt.legend()
plt.grid(True)
plt.show()

# -*- coding: utf-8 -*-
"""depth_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NNFH6ZyIF4SxqU805Jl5CkotXKKIXMyH
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
from torch.utils.data import DataLoader, TensorDataset


num_tasks = 10
batch_size = 32
learning_rate = 0.001
input_size = 784
neurons = 256
output = 10


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

mnist_train = datasets.MNIST(root='./data', train=True, download=True,
                             transform=transforms.ToTensor())
mnist_test = datasets.MNIST(root='./data', train=False, download=True,
                            transform=transforms.ToTensor())

X_train = mnist_train.data.view(-1, 784).float() / 255.0
y_train = mnist_train.targets
X_test = mnist_test.data.view(-1, 784).float() / 255.0
y_test = mnist_test.targets


permutations = [torch.tensor(np.random.permutation(input_size)) for i in range(num_tasks)]


def get_task_dataset(X, y, permutation):
    X_perm = X[:, permutation]          # reorder columns in one shot
    return TensorDataset(X_perm, y)

task_datasets = []
for curr_perm in permutations:
    train_dataset = get_task_dataset(X_train, y_train, curr_perm)
    test_dataset  = get_task_dataset(X_test,  y_test,  curr_perm)
    task_datasets.append((train_dataset, test_dataset))


class MLP(nn.Module):
    def __init__(self, input_size, neurons, output):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, neurons),
            nn.ReLU(),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Linear(neurons, output),
            nn.LogSoftmax(dim=1)
        )
    def forward(self, x):
        return self.net(x)

def train_task(model, train_loader, optimizer, criterion, epochs):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"   Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}")

def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return correct / total


model = MLP(input_size, neurons, output).to(device)
criterion = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)



task_id=0
for train_dataset, test_dataset in task_datasets:
    print(f"\n=== Training on Task {task_id+1} ===")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    if task_id==0:
        train_task(model, train_loader, optimizer, criterion, epochs=50)
    else:
        train_task(model, train_loader, optimizer, criterion, epochs=20)


    for i in range(task_id + 1):
        test_loader = DataLoader(task_datasets[i][1], batch_size=batch_size, shuffle=False)
        acc = evaluate(model, test_loader)
        print(f"Accuracy on Task {i+1}: {acc*100:.2f}%")
    task_id+=1

depth 3

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
from torch.utils.data import DataLoader, TensorDataset


num_tasks = 10
batch_size = 32
learning_rate = 0.001
input_size = 784
neurons = 256
output = 10


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

mnist_train = datasets.MNIST(root='./data', train=True, download=True,
                             transform=transforms.ToTensor())
mnist_test = datasets.MNIST(root='./data', train=False, download=True,
                            transform=transforms.ToTensor())

X_train = mnist_train.data.view(-1, 784).float() / 255.0
y_train = mnist_train.targets
X_test = mnist_test.data.view(-1, 784).float() / 255.0
y_test = mnist_test.targets


permutations = [torch.tensor(np.random.permutation(input_size)) for i in range(num_tasks)]


def get_task_dataset(X, y, permutation):
    X_perm = X[:, permutation]          # reorder columns in one shot
    return TensorDataset(X_perm, y)

task_datasets = []
for curr_perm in permutations:
    train_dataset = get_task_dataset(X_train, y_train, curr_perm)
    test_dataset  = get_task_dataset(X_test,  y_test,  curr_perm)
    task_datasets.append((train_dataset, test_dataset))


class MLP(nn.Module):
    def __init__(self, input_size, neurons, output):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
          nn.Linear(input_size, neurons),
            nn.ReLU(),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Linear(neurons, output),
            nn.LogSoftmax(dim=1)
        )
    def forward(self, x):
        return self.net(x)

def train_task(model, train_loader, optimizer, criterion, epochs):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"   Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}")

def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return correct / total


model = MLP(input_size, neurons, output).to(device)
criterion = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)



task_id=0
for train_dataset, test_dataset in task_datasets:
    print(f"\n=== Training on Task {task_id+1} ===")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    if task_id==0:
        train_task(model, train_loader, optimizer, criterion, epochs=50)
    else:
        train_task(model, train_loader, optimizer, criterion, epochs=20)


    for i in range(task_id + 1):
        test_loader = DataLoader(task_datasets[i][1], batch_size=batch_size, shuffle=False)
        acc = evaluate(model, test_loader)
        print(f"Accuracy on Task {i+1}: {acc*100:.2f}%")
    task_id+=1

"""depth 4"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

num_tasks = 10
batch_size = 32
learning_rate = 0.001
input_size = 784
neurons = 256
output = 10

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

mnist_train = datasets.MNIST(root='./data', train=True, download=True,
                             transform=transforms.ToTensor())
mnist_test = datasets.MNIST(root='./data', train=False, download=True,
                            transform=transforms.ToTensor())

X_train = mnist_train.data.view(-1, 784).float() / 255.0
y_train = mnist_train.targets
X_test = mnist_test.data.view(-1, 784).float() / 255.0
y_test = mnist_test.targets

permutations = [torch.tensor(np.random.permutation(input_size)) for i in range(num_tasks)]

def get_task_dataset(X, y, permutation):
    X_perm = X[:, permutation]          # reorder columns in one shot
    return TensorDataset(X_perm, y)

task_datasets = []
for curr_perm in permutations:
    train_dataset = get_task_dataset(X_train, y_train, curr_perm)
    test_dataset  = get_task_dataset(X_test,  y_test,  curr_perm)
    task_datasets.append((train_dataset, test_dataset))

class MLP(nn.Module):
    def __init__(self, input_size, neurons, output):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, neurons),
            nn.ReLU(),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Linear(neurons, output),
            nn.LogSoftmax(dim=1)
        )
    def forward(self, x):
        return self.net(x)

def train_task(model, train_loader, test_loader, optimizer, criterion, epochs, task_id, val_history):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # evaluate at the end of each epoch
        acc = evaluate(model, test_loader)
        val_history[task_id].append(acc * 100)

        print(f"   Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Val Acc: {acc*100:.2f}%")

def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return correct / total

model = MLP(input_size, neurons, output).to(device)
criterion = nn.NLLLoss()
optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)

# dictionary to store validation accuracies per epoch per task
val_history = {i: [] for i in range(num_tasks)}

task_id=0
for train_dataset, test_dataset in task_datasets:
    print(f"\n=== Training on Task {task_id+1} ===")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    if task_id==0:
        train_task(model, train_loader, test_loader, optimizer, criterion, epochs=50, task_id=task_id, val_history=val_history)
    else:
        train_task(model, train_loader, test_loader, optimizer, criterion, epochs=20, task_id=task_id, val_history=val_history)

    for i in range(task_id + 1):
        test_loader = DataLoader(task_datasets[i][1], batch_size=batch_size, shuffle=False)
        acc = evaluate(model, test_loader)
        print(f"Accuracy on Task {i+1}: {acc*100:.2f}%")
    task_id+=1


plt.figure(figsize=(10,6))
for task_id, acc_list in val_history.items():
    if len(acc_list) > 0:   # plot only trained tasks
        plt.plot(range(1, len(acc_list)+1), acc_list, label=f"Task {task_id+1}")

plt.xlabel("Epochs")
plt.ylabel("Validation Accuracy (%)")
plt.title("Validation Accuracy per Task")
plt.legend()
plt.grid(True)
plt.show()

"""depth 3 RMS prop"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import torch.nn.functional as F

num_tasks = 10
batch_size = 32
learning_rate = 0.001
input_size = 784
neurons = 256
output = 10

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

mnist_train = datasets.MNIST(root='./data', train=True, download=True,
                             transform=transforms.ToTensor())
mnist_test = datasets.MNIST(root='./data', train=False, download=True,
                            transform=transforms.ToTensor())

X_train = mnist_train.data.view(-1, 784).float() / 255.0
y_train = mnist_train.targets
X_test = mnist_test.data.view(-1, 784).float() / 255.0
y_test = mnist_test.targets

permutations = [torch.tensor(np.random.permutation(input_size)) for i in range(num_tasks)]

def get_task_dataset(X, y, permutation):
    X_perm = X[:, permutation]          # reorder columns in one shot
    return TensorDataset(X_perm, y)

task_datasets = []
for curr_perm in permutations:
    train_dataset = get_task_dataset(X_train, y_train, curr_perm)
    test_dataset  = get_task_dataset(X_test,  y_test,  curr_perm)
    task_datasets.append((train_dataset, test_dataset))

# -------- MLP without LogSoftmax --------
class MLP(nn.Module):
    def __init__(self, input_size, neurons, output):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, neurons),
            nn.ReLU(),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Linear(neurons, output)
        )
    def forward(self, x):
        return self.net(x)

# -------- Training with L1Loss --------
def train_task(model, train_loader, test_loader, optimizer, criterion, epochs, task_id, acc_matrix):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()

            outputs = model(batch_x)              # logits
            probs = F.softmax(outputs, dim=1)     # convert to probabilities

            # one-hot encode labels
            one_hot = F.one_hot(batch_y, num_classes=output).float()

            # compute L1 loss
            loss = criterion(probs, one_hot)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        acc = evaluate(model, test_loader)
        acc_matrix[task_id].append(acc * 100)
        print(f"   Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Val Acc: {acc*100:.2f}%")

def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            probs = F.softmax(outputs, dim=1)
            _, predicted = torch.max(probs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return correct / total

model = MLP(input_size, neurons, output).to(device)
criterion = nn.L1Loss()
optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)

# dictionary to store accuracies across tasks
acc_matrix = {i: [] for i in range(num_tasks)}

task_id = 0
for train_dataset, test_dataset in task_datasets:
    print(f"\n=== Training on Task {task_id+1} ===")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    if task_id == 0:
        train_task(model, train_loader, test_loader, optimizer, criterion, epochs=50, task_id=task_id, acc_matrix=acc_matrix)
    else:
        train_task(model, train_loader, test_loader, optimizer, criterion, epochs=20, task_id=task_id, acc_matrix=acc_matrix)

    for i in range(task_id + 1):
        test_loader = DataLoader(task_datasets[i][1], batch_size=batch_size, shuffle=False)
        acc = evaluate(model, test_loader)
        acc_matrix[i].append(acc * 100)
        print(f"Accuracy on Task {i+1}: {acc*100:.2f}%")
    task_id += 1

# ---- plotting ----
plt.figure(figsize=(10,6))
for i, acc_list in acc_matrix.items():
    if len(acc_list) > 0:
        plt.plot(range(i+1, num_tasks+1), acc_list, marker='o', label=f"Task {i+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curve Across Tasks (L1 Loss)")
plt.legend()
plt.grid(True)
plt.show()

"""L1 Loss"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

# -------------------------
# Settings
# -------------------------
num_tasks = 10
batch_size = 32
learning_rate = 0.001
input_size = 784
neurons = 256
output = 10

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# -------------------------
# Data
# -------------------------
mnist_train = datasets.MNIST(root='./data', train=True, download=True,
                             transform=transforms.ToTensor())
mnist_test = datasets.MNIST(root='./data', train=False, download=True,
                            transform=transforms.ToTensor())

X_train = mnist_train.data.view(-1, 784).float() / 255.0
y_train = mnist_train.targets
X_test = mnist_test.data.view(-1, 784).float() / 255.0
y_test = mnist_test.targets

permutations = [torch.tensor(np.random.permutation(input_size)) for i in range(num_tasks)]

def get_task_dataset(X, y, permutation):
    X_perm = X[:, permutation]          # reorder columns in one shot
    return TensorDataset(X_perm, y)

task_datasets = []
for curr_perm in permutations:
    train_dataset = get_task_dataset(X_train, y_train, curr_perm)
    test_dataset  = get_task_dataset(X_test,  y_test,  curr_perm)
    task_datasets.append((train_dataset, test_dataset))

# -------------------------
# Model
# -------------------------
class MLP(nn.Module):
    def __init__(self, input_size, neurons, output):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, neurons),
            nn.ReLU(),
            nn.Linear(neurons, neurons),
            nn.ReLU(),
            nn.Linear(neurons, output)   # no log-softmax here because Huber works on raw logits
        )
    def forward(self, x):
        return self.net(x)

# -------------------------
# Training / Evaluation
# -------------------------
def train_task(model, train_loader, test_loader, optimizer, criterion, epochs, task_id, val_history):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)

            optimizer.zero_grad()
            outputs = model(batch_x)

            # Convert labels to one-hot for regression-style losses like Huber
            batch_y_onehot = torch.nn.functional.one_hot(batch_y, num_classes=output).float()

            loss = criterion(outputs, batch_y_onehot)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # validation accuracy
        acc = evaluate(model, test_loader)
        val_history[task_id].append(acc * 100)
        print(f"   Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Val Acc: {acc*100:.2f}%")

def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return correct / total

# -------------------------
# Setup Model + Huber Loss
# -------------------------
model = MLP(input_size, neurons, output).to(device)
criterion = nn.HuberLoss(delta=1.0)   # ðŸ”¹ Huber loss
optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
# -------------------------
# Store validation accuracies per epoch
# -------------------------
val_history = {i: [] for i in range(num_tasks)}   # initialize once

task_id = 0
for train_dataset, test_dataset in task_datasets:
    print(f"\n=== Training on Task {task_id+1} ===")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    if task_id == 0:
        train_task(model, train_loader, DataLoader(test_dataset, batch_size=batch_size),
                   optimizer, criterion, epochs=50, task_id=task_id, val_history=val_history)
    else:
        train_task(model, train_loader, DataLoader(test_dataset, batch_size=batch_size),
                   optimizer, criterion, epochs=20, task_id=task_id, val_history=val_history)

    # Evaluate on all tasks up to now
    for i in range(task_id + 1):
        test_loader = DataLoader(task_datasets[i][1], batch_size=batch_size, shuffle=False)
        acc = evaluate(model, test_loader)
        acc_matrix[i].append(acc * 100)
        print(f"Accuracy on Task {i+1}: {acc*100:.2f}%")

    task_id += 1


# -------------------------
# Plot Forgetting Curve
# -------------------------
plt.figure(figsize=(10,6))
for i, acc_list in acc_matrix.items():
    if len(acc_list) > 0:
        plt.plot(range(i+1, num_tasks+1), acc_list, marker='o', label=f"Task {i+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curve Across Tasks (Huber Loss)")
plt.legend()
plt.grid(True)
plt.show()

"""Huber loss"""

import matplotlib.pyplot as plt
import numpy as np

# ---- Hardcoded accuracy matrix from your output ----
acc_matrix = np.array([
    [98.19,  0,      0,      0,      0,      0,      0,      0,      0,      0],   # after Task 1
    [88.25, 97.74,   0,      0,      0,      0,      0,      0,      0,      0],   # after Task 2
    [61.41, 82.05, 97.42,    0,      0,      0,      0,      0,      0,      0],   # after Task 3
    [46.21, 40.10, 77.21, 97.19,    0,      0,      0,      0,      0,      0],   # after Task 4
    [28.37, 31.82, 46.73, 79.40, 97.44,     0,      0,      0,      0,      0],   # after Task 5
    [24.61, 20.29, 28.32, 47.34, 74.67, 97.13,     0,      0,      0,      0],   # after Task 6
    [18.95, 16.77, 19.66, 26.49, 41.46, 76.63, 96.95,     0,      0,      0],   # after Task 7
    [16.83, 13.82, 19.15, 21.41, 24.54, 41.95, 75.69, 97.23,     0,      0],   # after Task 8
    [14.18,  9.43, 17.76, 11.61, 19.91, 23.42, 49.18, 83.88, 97.10,     0],   # after Task 9
    [20.45, 12.39, 12.78, 14.49, 25.26, 32.24, 39.76, 56.23, 82.85, 96.97]   # after Task 10
])

T = acc_matrix.shape[0]

# ---- Metrics ----
ACC = np.mean(acc_matrix[-1])

# Backward Transfer (BWT)
BWT = np.mean([acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)])

# Task-wise BWT (TBWT) - per task forgetting
TBWT = [acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)]

# Cumulative BWT (CBWT) - average of TBWT
CBWT = np.mean(TBWT)

# ---- Print Results ----
print(f"ACC (Final Average Accuracy): {ACC:.2f}%")
print(f"BWT (Backward Transfer): {BWT:.2f}%")
print(f"TBWT (Task-wise BWT): {TBWT}")
print(f"CBWT (Cumulative BWT): {CBWT:.2f}%")

# ---- Plot Forgetting Curves ----
plt.figure(figsize=(12,7))
for task in range(T):
    valid_accs = acc_matrix[task:, task]
    plt.plot(range(task+1, T+1), valid_accs, marker='o', label=f"Task {task+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curves for Each Task")
plt.legend()
plt.grid(True)
plt.show()

"""Plot depth 3"""

import matplotlib.pyplot as plt
import numpy as np

# ---- Hardcoded accuracy matrix from your output ----
# === Accuracy matrix reconstructed from your log ===
acc_matrix = np.array([
    [97.98,   0,      0,      0,      0,      0,      0,      0,      0,      0],  # after Task 1
    [44.29, 97.44,   0,      0,      0,      0,      0,      0,      0,      0],  # after Task 2
    [42.40, 79.98, 97.54,    0,      0,      0,      0,      0,      0,      0],  # after Task 3
    [23.92, 36.51, 68.38, 97.26,    0,      0,      0,      0,      0,      0],  # after Task 4
    [21.64, 21.12, 28.64, 80.76, 97.42,     0,      0,      0,      0,      0],  # after Task 5
    [21.20, 15.23, 19.52, 45.04, 67.34, 97.37,     0,      0,      0,      0],  # after Task 6
    [24.78, 13.06, 11.41, 31.33, 45.47, 82.32, 96.53,     0,      0,      0],  # after Task 7
    [19.12, 18.46, 15.96, 20.13, 39.25, 51.91, 82.98, 96.84,     0,      0],  # after Task 8
    [20.23, 20.22, 12.55, 13.81, 35.85, 44.58, 53.68, 81.19, 96.70,     0],  # after Task 9
    [21.21, 16.88, 15.05, 14.35, 26.28, 37.24, 52.77, 57.96, 81.55, 97.14]   # after Task 10
])


T = acc_matrix.shape[0]

# ---- Metrics ----
ACC = np.mean(acc_matrix[-1])

# Backward Transfer (BWT)
BWT = np.mean([acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)])

# Task-wise BWT (TBWT) - per task forgetting
TBWT = [acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)]

# Cumulative BWT (CBWT) - average of TBWT
CBWT = np.mean(TBWT)

# ---- Print Results ----
print(f"ACC (Final Average Accuracy): {ACC:.2f}%")
print(f"BWT (Backward Transfer): {BWT:.2f}%")
print(f"TBWT (Task-wise BWT): {TBWT}")
print(f"CBWT (Cumulative BWT): {CBWT:.2f}%")

# ---- Plot Forgetting Curves ----
plt.figure(figsize=(12,7))
for task in range(T):
    valid_accs = acc_matrix[task:, task]
    plt.plot(range(task+1, T+1), valid_accs, marker='o', label=f"Task {task+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curves for Each Task")
plt.legend()
plt.grid(True)
plt.show()

"""Plot depth 4"""

import matplotlib.pyplot as plt
import numpy as np

# ---- Hardcoded accuracy matrix from your output ----
# === Accuracy matrix reconstructed from your log ===
acc_matrix = np.array([
    [98.20,   0,      0,      0,      0,      0,      0,      0,      0,      0],  # after Task 1
    [79.61, 97.31,   0,      0,      0,      0,      0,      0,      0,      0],  # after Task 2
    [43.88, 49.42, 97.90,    0,      0,      0,      0,      0,      0,      0],  # after Task 3
    [30.78, 39.11, 73.39, 97.66,    0,      0,      0,      0,      0,      0],  # after Task 4
    [18.16, 24.79, 40.20, 62.88, 97.49,     0,      0,      0,      0,      0],  # after Task 5
    [27.31, 26.41, 35.41, 40.80, 74.30, 97.52,     0,      0,      0,      0],  # after Task 6
    [17.35, 16.70, 22.70, 33.68, 43.66, 71.52, 97.25,     0,      0,      0],  # after Task 7
    [15.31, 14.34, 18.59, 28.29, 35.47, 57.04, 84.66, 97.51,     0,      0],  # after Task 8
    [14.54, 17.51, 15.39, 24.60, 28.66, 42.85, 60.24, 82.68, 97.43,     0],  # after Task 9
    [14.12, 15.82, 14.32, 19.29, 18.13, 31.72, 33.84, 52.53, 76.54, 97.11]   # after Task 10
])


T = acc_matrix.shape[0]

# ---- Metrics ----
ACC = np.mean(acc_matrix[-1])

# Backward Transfer (BWT)
BWT = np.mean([acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)])

# Task-wise BWT (TBWT) - per task forgetting
TBWT = [acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)]

# Cumulative BWT (CBWT) - average of TBWT
CBWT = np.mean(TBWT)

# ---- Print Results ----
print(f"ACC (Final Average Accuracy): {ACC:.2f}%")
print(f"BWT (Backward Transfer): {BWT:.2f}%")
print(f"TBWT (Task-wise BWT): {TBWT}")
print(f"CBWT (Cumulative BWT): {CBWT:.2f}%")

# ---- Plot Forgetting Curves ----
plt.figure(figsize=(12,7))
for task in range(T):
    valid_accs = acc_matrix[task:, task]
    plt.plot(range(task+1, T+1), valid_accs, marker='o', label=f"Task {task+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curves for Each Task")
plt.legend()
plt.grid(True)
plt.show()

"""RMSprop"""

import matplotlib.pyplot as plt
import numpy as np

# ---- Hardcoded accuracy matrix from your output ----
# === Accuracy matrix reconstructed from your log ===
acc_matrix = np.array([
    [97.81,   0,      0,      0,      0,      0,      0,      0,      0,      0],   # after Task 1
    [94.96, 97.16,   0,      0,      0,      0,      0,      0,      0,      0],   # after Task 2
    [75.04, 88.82, 85.38,    0,      0,      0,      0,      0,      0,      0],   # after Task 3
    [62.53, 79.13, 81.65, 96.11,    0,      0,      0,      0,      0,      0],   # after Task 4
    [48.87, 64.31, 72.07, 90.98, 95.26,     0,      0,      0,      0,      0],   # after Task 5
    [41.70, 44.38, 57.06, 78.07, 88.41, 95.14,     0,      0,      0,      0],   # after Task 6
    [38.03, 34.62, 47.61, 59.90, 75.89, 90.20, 94.36,     0,      0,      0],   # after Task 7
    [27.53, 28.78, 42.41, 53.53, 58.74, 77.00, 86.99, 94.27,     0,      0],   # after Task 8
    [27.94, 26.35, 43.54, 49.97, 46.53, 62.95, 76.24, 92.04, 94.82,     0],   # after Task 9
    [24.49, 26.39, 42.36, 36.06, 43.76, 56.68, 72.14, 75.91, 87.30, 93.19]   # after Task 10
])


T = acc_matrix.shape[0]

# ---- Metrics ----
ACC = np.mean(acc_matrix[-1])

# Backward Transfer (BWT)
BWT = np.mean([acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)])

# Task-wise BWT (TBWT) - per task forgetting
TBWT = [acc_matrix[-1, i] - acc_matrix[i, i] for i in range(T-1)]

# Cumulative BWT (CBWT) - average of TBWT
CBWT = np.mean(TBWT)

# ---- Print Results ----
print(f"ACC (Final Average Accuracy): {ACC:.2f}%")
print(f"BWT (Backward Transfer): {BWT:.2f}%")
print(f"TBWT (Task-wise BWT): {TBWT}")
print(f"CBWT (Cumulative BWT): {CBWT:.2f}%")

# ---- Plot Forgetting Curves ----
plt.figure(figsize=(12,7))
for task in range(T):
    valid_accs = acc_matrix[task:, task]
    plt.plot(range(task+1, T+1), valid_accs, marker='o', label=f"Task {task+1}")

plt.xlabel("After Training Task k")
plt.ylabel("Accuracy (%)")
plt.title("Forgetting Curves for Each Task")
plt.legend()
plt.grid(True)
plt.show()

"""L1"""
