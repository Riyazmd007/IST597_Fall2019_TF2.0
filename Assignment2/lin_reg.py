# -*- coding: utf-8 -*-
"""loss.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NBZbhLr_W9Hvyw2OkuDg-KQoT8S_Ypi0
"""

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)


# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []

for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)


# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []

for i in range(train_steps):
    y_pred = prediction(X)
    loss = l1_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []

for i in range(train_steps):
    y_pred = prediction(X)
    loss = huber_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.01
train_steps = 1000
loss_history = []

for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.01
train_steps = 1000
loss_history = []

for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.1
train_steps = 1000
loss_history = []

for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.01
train_steps = 1000
loss_history = []
patience=25
best_loss = float('inf')
wait = 0

for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()
     # Patience scheduling
    if loss.item() < round(best_loss,2):
        best_loss = loss.item()
        wait = 0
    else:
        wait += 1
        if wait >= patience:
            learning_rate = learning_rate/ 2
            print(f"Epoch {i}: Loss didnt change. Reducing learning rate to {learning_rate}")
            wait = 0
    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []


for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 2000
loss_history = []


for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.tensor([[100.0]], requires_grad=True)
b = torch.tensor([[50.0]], requires_grad=True)



def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []


for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.tensor([[100.0]], requires_grad=True)
b = torch.tensor([[50.0]], requires_grad=True)




def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []


for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.tensor([[100.0]], requires_grad=True)
b = torch.tensor([[50.0]], requires_grad=True)




def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 3000
loss_history = []


for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise


W = torch.tensor([[100.0]], requires_grad=True)
b = torch.tensor([[50.0]], requires_grad=True)




def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)




# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 6000
loss_history = []


for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 10 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

"""Noise questions"""

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10

noise_level = 5.0   # larger noise
noise = torch.randn(NUM_EXAMPLES, 1) * noise_level
y = 3 * X + 2 + noise

y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)


# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []

for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10

# Uniform noise
noise = (torch.rand(NUM_EXAMPLES, 1) - 0.5) * 2  # [-1, 1]

# Laplace noise
#noise = torch.distributions.laplace.Laplace(0,1).sample((NUM_EXAMPLES,1))

y = 3 * X + 2 + noise

y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)


# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []

for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt


NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10

# Uniform noise
#noise = (torch.rand(NUM_EXAMPLES, 1) - 0.5) * 2  # [-1, 1]

# Laplace noise
noise = torch.distributions.laplace.Laplace(0,1).sample((NUM_EXAMPLES,1))

y = 3 * X + 2 + noise

y = 3 * X + 2 + noise


W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)


def prediction(x):
    return x * W + b

# -------------------------------
# Loss
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    # element-wise huber loss
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)


# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []

for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt

NUM_EXAMPLES = 10000
train_steps = 1000
N = 10  # Apply noise every N steps

# -------------------------------
# Data
# -------------------------------
X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
y = 3 * X + 2

# -------------------------------
# Model Parameters
# -------------------------------
W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)

# -------------------------------
# Prediction
# -------------------------------
def prediction(x):
    return x * W + b

# -------------------------------
# Loss Functions
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)

# -------------------------------
# Training Setup
# -------------------------------
lr_base = 0.001
learning_rate = lr_base
loss_history = []

for i in range(train_steps):
    # -------------------------------
    # Apply data noise per N steps
    # -------------------------------
    if i % N == 0:
        X_noisy = X + torch.randn(NUM_EXAMPLES, 1) * 0.1
        y_noisy = y + torch.randn(NUM_EXAMPLES, 1) * 0.1

    # -------------------------------
    # Forward pass
    # -------------------------------
    y_pred = prediction(X_noisy)
    loss = squared_loss(y_noisy, y_pred)
    loss_history.append(loss.item())

    # -------------------------------
    # Backpropagation
    # -------------------------------
    loss.backward()
    with torch.no_grad():
        # -------------------------------
        # Update weights
        # -------------------------------
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad

        # -------------------------------
        # Zero gradients
        # -------------------------------
        W.grad.zero_()
        b.grad.zero_()

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}, LR: {learning_rate:.6f}")

# -------------------------------
# Plot Results
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X_noisy.numpy(), y_noisy.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X_noisy.numpy(), prediction(X_noisy).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt

NUM_EXAMPLES = 10000
train_steps = 1000
N = 10  # Apply noise every N steps

# -------------------------------
# Data
# -------------------------------
X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
y = 3 * X + 2

# -------------------------------
# Model Parameters
# -------------------------------
W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)

# -------------------------------
# Prediction
# -------------------------------
def prediction(x):
    return x * W + b

# -------------------------------
# Loss Functions
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)

# -------------------------------
# Training Setup
# -------------------------------
lr_base = 0.001
learning_rate = lr_base
loss_history = []

for i in range(train_steps):

    # -------------------------------
    # Forward pass
    # -------------------------------
    y_pred = prediction(X_noisy)
    loss = squared_loss(y_noisy, y_pred)
    loss_history.append(loss.item())

    # -------------------------------
    # Backpropagation
    # -------------------------------
    loss.backward()
    with torch.no_grad():
        # -------------------------------
        # Update weights
        # -------------------------------
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad

        # -------------------------------
        # Add weight noise per N steps
        # -------------------------------
        if i % N == 0:
            W += torch.randn(1, 1) * 0.01
            b += torch.randn(1, 1) * 0.01

        # -------------------------------
        # Zero gradients
        # -------------------------------
        W.grad.zero_()
        b.grad.zero_()

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}, LR: {learning_rate:.6f}")

# -------------------------------
# Plot Results
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X_noisy.numpy(), y_noisy.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X_noisy.numpy(), prediction(X_noisy).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt

NUM_EXAMPLES = 10000
train_steps = 1000
N = 10  # Apply noise every N steps

# -------------------------------
# Data
# -------------------------------
X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
y = 3 * X + 2

# -------------------------------
# Model Parameters
# -------------------------------
W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)

# -------------------------------
# Prediction
# -------------------------------
def prediction(x):
    return x * W + b

# -------------------------------
# Loss Functions
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)

# -------------------------------
# Training Setup
# -------------------------------
lr_base = 0.001
learning_rate = lr_base
loss_history = []

for i in range(train_steps):

    # -------------------------------
    # Forward pass
    # -------------------------------
    y_pred = prediction(X_noisy)
    loss = squared_loss(y_noisy, y_pred)
    loss_history.append(loss.item())

    # -------------------------------
    # Backpropagation
    # -------------------------------
    loss.backward()
    with torch.no_grad():
        # -------------------------------
        # Update weights
        # -------------------------------
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad


        # -------------------------------
        # Add learning rate noise per N steps
        # -------------------------------
        if i % N == 0:
            learning_rate = lr_base + torch.randn(1).item() * 0.0001

        # -------------------------------
        # Zero gradients
        # -------------------------------
        W.grad.zero_()
        b.grad.zero_()

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}, LR: {learning_rate:.6f}")

# -------------------------------
# Plot Results
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X_noisy.numpy(), y_noisy.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X_noisy.numpy(), prediction(X_noisy).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

"""Seed"""

import torch
import matplotlib.pyplot as plt

# -------------------------------
# Seed for reproducibility
# -------------------------------
# Convert first name "Riyazuddin" to ASCII sum as seed
name = "Riyazuddin"
seed = 1059
torch.manual_seed(seed)

NUM_EXAMPLES = 10000

X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)

y = 3 * X + 2 + noise

W = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, 1, requires_grad=True)

def prediction(x):
    return x * W + b

# -------------------------------
# Loss Functions
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []

for i in range(train_steps):
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, W: {W.item():.4f}, b: {b.item():.4f}")

# -------------------------------
# Plot
# -------------------------------
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.scatter(X.numpy(), y.numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.numpy(), prediction(X).detach().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

"""GPU vs CPU"""

import torch
import matplotlib.pyplot as plt
import time
import pandas as pd

# -------------------------------
# Device setup (CPU or GPU)
# -------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# -------------------------------
# Data
# -------------------------------
NUM_EXAMPLES = 10000
X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise

# Move data to device
X = X.to(device)
y = y.to(device)

# -------------------------------
# Initialize weights and bias
# -------------------------------
W = torch.randn(1, 1, requires_grad=True, device=device)
b = torch.randn(1, 1, requires_grad=True, device=device)

# -------------------------------
# Prediction function
# -------------------------------
def prediction(x):
    return x * W + b

# -------------------------------
# Loss functions
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)

# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []
epoch_times = []

for i in range(train_steps):
    start_time = time.time()  # Start timing epoch

    # Forward pass
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    # Backpropagation
    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    end_time = time.time()  # End timing epoch
    epoch_times.append(end_time - start_time)

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, Time for step: {end_time - start_time:.5f}s, W: {W.item():.4f}, b: {b.item():.4f}")

avg_epoch_time = sum(epoch_times) / len(epoch_times)
print(f"Average time per epoch: {avg_epoch_time:.5f}s")

# -------------------------------
# Save epoch-wise report
# -------------------------------
df = pd.DataFrame({
    "Epoch": list(range(train_steps)),
    "Loss": loss_history,
    "Time_per_epoch": epoch_times
})
df.to_csv("epoch_times.csv", index=False)

# -------------------------------
# Plot results
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.cpu().numpy(), y.cpu().numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.cpu().numpy(), prediction(X).detach().cpu().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt
import time
import pandas as pd

# -------------------------------
# Device setup (CPU or GPU)
# -------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# -------------------------------
# Data
# -------------------------------
NUM_EXAMPLES = 10000
X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise

# Move data to device
X = X.to(device)
y = y.to(device)

# -------------------------------
# Initialize weights and bias
# -------------------------------
W = torch.randn(1, 1, requires_grad=True, device=device)
b = torch.randn(1, 1, requires_grad=True, device=device)

# -------------------------------
# Prediction function
# -------------------------------
def prediction(x):
    return x * W + b

# -------------------------------
# Loss functions
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)

# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []
epoch_times = []

for i in range(train_steps):
    start_time = time.time()  # Start timing epoch

    # Forward pass
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    # Backpropagation
    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    end_time = time.time()  # End timing epoch
    epoch_times.append(end_time - start_time)

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, Time for step: {end_time - start_time:.5f}s, W: {W.item():.4f}, b: {b.item():.4f}")

avg_epoch_time = sum(epoch_times) / len(epoch_times)
print(f"Average time per epoch: {avg_epoch_time:.5f}s")

# -------------------------------
# Save epoch-wise report
# -------------------------------
df = pd.DataFrame({
    "Epoch": list(range(train_steps)),
    "Loss": loss_history,
    "Time_per_epoch": epoch_times
})
df.to_csv("epoch_times.csv", index=False)

# -------------------------------
# Plot results
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.cpu().numpy(), y.cpu().numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.cpu().numpy(), prediction(X).detach().cpu().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()

import torch
import matplotlib.pyplot as plt
import time
import pandas as pd

# -------------------------------
# Device setup (CPU or GPU)
# -------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# -------------------------------
# Data
# -------------------------------
NUM_EXAMPLES = 10000
X = torch.rand(NUM_EXAMPLES, 1, dtype=torch.float32) * 10
noise = torch.randn(NUM_EXAMPLES, 1, dtype=torch.float32)
y = 3 * X + 2 + noise

# Move data to device
X = X.to(device)
y = y.to(device)

# -------------------------------
# Initialize weights and bias
# -------------------------------
W = torch.randn(1, 1, requires_grad=True, device=device)
b = torch.randn(1, 1, requires_grad=True, device=device)

# -------------------------------
# Prediction function
# -------------------------------
def prediction(x):
    return x * W + b

# -------------------------------
# Loss functions
# -------------------------------
def squared_loss(y_true, y_pred):
    return torch.mean((y_true - y_pred)**2)

def l1_loss(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred))

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    cond = (error.abs() <= delta).float()
    loss = 0.5 * (error**2) * cond + delta * (error.abs() - 0.5 * delta) * (1 - cond)
    return torch.mean(loss)

# -------------------------------
# Training
# -------------------------------
learning_rate = 0.001
train_steps = 1000
loss_history = []
epoch_times = []

for i in range(train_steps):
    start_time = time.time()  # Start timing epoch

    # Forward pass
    y_pred = prediction(X)
    loss = squared_loss(y, y_pred)
    loss_history.append(loss.item())

    # Backpropagation
    loss.backward()
    with torch.no_grad():
        W -= learning_rate * W.grad
        b -= learning_rate * b.grad
        W.grad.zero_()
        b.grad.zero_()

    end_time = time.time()  # End timing epoch
    epoch_times.append(end_time - start_time)

    if i % 100 == 0:
        print(f"Step {i}, Loss: {loss.item():.4f}, Time for step: {end_time - start_time:.5f}s, W: {W.item():.4f}, b: {b.item():.4f}")

avg_epoch_time = sum(epoch_times) / len(epoch_times)
print(f"Average time per epoch: {avg_epoch_time:.5f}s")

# -------------------------------
# Save epoch-wise report
# -------------------------------
df = pd.DataFrame({
    "Epoch": list(range(train_steps)),
    "Loss": loss_history,
    "Time_per_epoch": epoch_times
})
df.to_csv("epoch_times.csv", index=False)

# -------------------------------
# Plot results
# -------------------------------
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X.cpu().numpy(), y.cpu().numpy(), color='blue', alpha=0.5, label='Data')
plt.plot(X.cpu().numpy(), prediction(X).detach().cpu().numpy(), color='red', label='Regression Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression Fit")
plt.legend()

plt.subplot(1,2,2)
plt.plot(loss_history)
plt.title("Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.show()
